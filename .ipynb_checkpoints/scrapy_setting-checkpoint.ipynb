{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy, beautifulsoup4\n",
    "https://youtu.be/LQS6QfDQ9RA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pip install scrapy\n",
    "2. error> pip uninstall scrapy\n",
    "3. %ls %cd.. 안불편한 경로로\n",
    "4. !scrapy startproject gitlab\n",
    "5. \\gitlab\\gitlab\\spiders\n",
    "    - %cd gitlab %cd gitlab %cd spiders %ls\n",
    "6. import scrapy ...\n",
    "    - https://doc.scrapy.org/en/latest/intro/tutorial.html\n",
    "7. %save py로만들파일명 line번호\n",
    "8. !scrapy crawl class첫줄에있는이름\n",
    "9. 8번 실행위치 폴더 확인\n",
    "\n",
    "---\n",
    "https://about.gitlab.com/handbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notebook을 파일로 저장\n",
    "!jupyter nbconvert --to script 노트북[경로및]이름.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install scrapy\n",
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 2468-0A05\n",
      "\n",
      " E:\\github\\dataAnal_py_basic 디렉터리\n",
      "\n",
      "2018-03-14  오후 06:26    <DIR>          .\n",
      "2018-03-14  오후 06:26    <DIR>          ..\n",
      "2018-03-14  오후 06:22    <DIR>          .ipynb_checkpoints\n",
      "2018-03-14  오후 06:26            25,503 crawling.ipynb\n",
      "2018-02-19  오전 09:41    <DIR>          dss_bicycleShare\n",
      "2018-02-19  오전 09:41    <DIR>          dss_note\n",
      "2018-02-19  오전 09:41    <DIR>          dss_sentimentMovie\n",
      "2018-02-19  오전 09:41    <DIR>          dss_syntax\n",
      "2018-02-19  오전 09:41    <DIR>          dss_testPython\n",
      "2018-02-26  오전 09:35    <DIR>          dss_titanic\n",
      "2018-02-19  오전 09:41    <DIR>          dss_xgBoost\n",
      "2018-02-13  오전 09:15    <DIR>          housePrice\n",
      "2018-03-13  오후 12:09                16 j.bat\n",
      "2018-02-28  오후 07:42    <DIR>          my_txtSummarize\n",
      "2018-02-28  오후 07:32    <DIR>          otherPackage\n",
      "2018-03-14  오후 06:19    <DIR>          pandas\n",
      "2018-03-14  오후 06:22    <DIR>          popularPkg\n",
      "2018-03-12  오전 09:21    <DIR>          syntax1\n",
      "               2개 파일              25,519 바이트\n",
      "              16개 디렉터리  225,969,750,016 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'gitlab', using template directory 'c:\\\\users\\\\kitcoop\\\\anaconda3\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in:\n",
      "    E:\\github\\dataAnal_py_basic\\gitlab\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd gitlab\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "# !scrapy startproject gitlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 2468-0A05\n",
      "\n",
      " E:\\github\\dataAnal_py_basic\\gitlab\\gitlab\\spiders 디렉터리\n",
      "\n",
      "2018-03-14  오후 06:56    <DIR>          .\n",
      "2018-03-14  오후 06:56    <DIR>          ..\n",
      "2018-03-14  오후 06:28               161 __init__.py\n",
      "2018-03-14  오후 06:28    <DIR>          __pycache__\n",
      "2018-03-14  오후 06:50               592 quotes_spider.py\n",
      "               2개 파일                 753 바이트\n",
      "               3개 디렉터리  225,968,852,992 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "# %cd gitlab\n",
    "# %cd gitlab\n",
    "# %cd spiders\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `quotes_spider.py`:\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class QuotesSpider(scrapy.Spider):\n",
      "    name = \"quotes\"\n",
      "\n",
      "    def start_requests(self):\n",
      "        urls = [\n",
      "            'http://quotes.toscrape.com/page/1/',\n",
      "            'http://quotes.toscrape.com/page/2/',\n",
      "        ]\n",
      "        for url in urls:\n",
      "            yield scrapy.Request(url=url, callback=self.parse)\n",
      "\n",
      "    def parse(self, response):\n",
      "        page = response.url.split(\"/\")[-2]\n",
      "        filename = 'quotes-%s.html' % page\n",
      "        with open(filename, 'wb') as f:\n",
      "            f.write(response.body)\n",
      "        self.log('Saved file %s' % filename)\n"
     ]
    }
   ],
   "source": [
    "%save quotes_spider 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 2468-0A05\n",
      "\n",
      " E:\\github\\dataAnal_py_basic\\gitlab\\gitlab\\spiders 디렉터리\n",
      "\n",
      "2018-03-14  오후 06:56    <DIR>          .\n",
      "2018-03-14  오후 06:56    <DIR>          ..\n",
      "2018-03-14  오후 06:28               161 __init__.py\n",
      "2018-03-14  오후 06:28    <DIR>          __pycache__\n",
      "2018-03-14  오후 06:50               592 quotes_spider.py\n",
      "               2개 파일                 753 바이트\n",
      "               3개 디렉터리  225,968,852,992 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-14 18:57:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: gitlab)\n",
      "2018-03-14 18:57:50 [scrapy.utils.log] INFO: Versions: lxml 4.1.0.0, libxml2 2.9.4, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.0.2n  7 Dec 2017), cryptography 2.0.3, Platform Windows-7-6.1.7601-SP1\n",
      "2018-03-14 18:57:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gitlab', 'NEWSPIDER_MODULE': 'gitlab.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gitlab.spiders']}\n",
      "2018-03-14 18:57:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2018-03-14 18:57:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-03-14 18:57:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-03-14 18:57:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-03-14 18:57:50 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-03-14 18:57:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-03-14 18:57:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2018-03-14 18:57:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
      "2018-03-14 18:57:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2018-03-14 18:57:52 [quotes] DEBUG: Saved file quotes-1.html\n",
      "2018-03-14 18:57:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2018-03-14 18:57:52 [quotes] DEBUG: Saved file quotes-2.html\n",
      "2018-03-14 18:57:52 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-03-14 18:57:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 678,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 5976,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 3, 14, 9, 57, 52, 447980),\n",
      " 'log_count/DEBUG': 6,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 3,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2018, 3, 14, 9, 57, 50, 965977)}\n",
      "2018-03-14 18:57:52 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 2468-0A05\n",
      "\n",
      " E:\\github\\dataAnal_py_basic\\gitlab\\gitlab\\spiders 디렉터리\n",
      "\n",
      "2018-03-14  오후 06:57    <DIR>          .\n",
      "2018-03-14  오후 06:57    <DIR>          ..\n",
      "2018-03-14  오후 06:28               161 __init__.py\n",
      "2018-03-14  오후 06:57    <DIR>          __pycache__\n",
      "2018-03-14  오후 06:50               592 quotes_spider.py\n",
      "2018-03-14  오후 06:57            11,053 quotes-1.html\n",
      "2018-03-14  오후 06:57            13,734 quotes-2.html\n",
      "               4개 파일              25,540 바이트\n",
      "               3개 디렉터리  225,968,771,072 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CMD\n",
    "scrapy shell \"http://quotes.toscrape.com/page/1/\"\n",
    "\n",
    "---\n",
    "\n",
    "windows> \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
